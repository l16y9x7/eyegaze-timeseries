{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "#####################\n",
    "# Load Data\n",
    "#####################\n",
    "\n",
    "# load data set_v1\n",
    "excel_file = 'image-manipulation (timeseries)/Caldwell_ImageManipulation-EyeGaze_DataSetCombined.xlsx'\n",
    "olddata = pd.read_excel(excel_file, sheet_name=1)\n",
    "# rename the labels of dataset_v1\n",
    "olddata.rename(columns={'participant':'Participant_ID'}, inplace=True)\n",
    "olddata.rename(columns={'image':'Image_ID'}, inplace=True)\n",
    "# load data set_2\n",
    "excel_file = 'image-manipulation_v2/Caldwell_Manip_Images_10-14_TimeSeries.csv'\n",
    "predata = pd.read_csv(excel_file)\n",
    "# merge two dataset by the 'Participant_ID','Image_ID'\n",
    "predata2 = pd.merge(predata,olddata,how='left', on=['Participant_ID','Image_ID'])\n",
    "# remove the useless column from dataset_v1\n",
    "newdata = predata2.iloc[:, [0,1,2,3,4,5,6,7,8,13,14]]\n",
    "# sort the dataset by start time\n",
    "data = newdata.sort_values(by=['Start Time'])\n",
    "\n",
    "# normalization the dataset between 0 and 1 by Min-Max scaling\n",
    "# https://sebastianraschka.com/Articles/2014_about_feature_scaling.html#about-min-max-scaling\n",
    "for i in range(0, data.shape[1] - 2):\n",
    "    dataMax = data[[data.columns[i]]].max()\n",
    "    dataMin = data[[data.columns[i]]].min()\n",
    "    data[[data.columns[i]]] = (data[[data.columns[i]]] - dataMin)/ (dataMax - dataMin)\n",
    "\n",
    "# use the Image_ID 10-12 (after normalization 0-0.5)as the train data,13 (0.75) as validate data and 14 (1.0) as test data\n",
    "traindata = data.loc[data['Image_ID'] < 0.75]\n",
    "\n",
    "validatedata = data.loc[data['Image_ID'] == 0.75]\n",
    "\n",
    "testdata = data.loc[data['Image_ID'] == 1.0]\n",
    "\n",
    "train_input = traindata.iloc[:, :9]\n",
    "train_target = traindata.iloc[:, -1]\n",
    "\n",
    "validate_input = validatedata.iloc[:, :9]\n",
    "validate_target = validatedata.iloc[:, -1]\n",
    "\n",
    "test_input = testdata.iloc[:, :9]\n",
    "test_target = testdata.iloc[:, -1]\n",
    "\n",
    "X_train = Variable(torch.Tensor(train_input.values).float())\n",
    "Y_train = Variable(torch.Tensor(train_target.values).long())\n",
    "\n",
    "X_validate = Variable(torch.Tensor(validate_input.values).float())\n",
    "Y_validate = Variable(torch.Tensor(validate_target.values).long())\n",
    "\n",
    "X_test = Variable(torch.Tensor(test_input.values).float())\n",
    "Y_test = Variable(torch.Tensor(test_target.values).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(9, 10, batch_first=True)\n",
       "  (fc): Linear(in_features=10, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################\n",
    "# LSTM model\n",
    "#####################\n",
    "\n",
    "n_features = 9\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.1\n",
    "batch_size = 1\n",
    "#num_hidden_layer = 1\n",
    "input_dim = n_features\n",
    "hidden_dim = 10\n",
    "output_dim=3\n",
    "num_layers=1\n",
    "\n",
    "# reference example LSTM\n",
    "# https://www.jessicayung.com/lstms-for-time-series-in-pytorch/\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim, num_layers, batch_first=True):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "        \n",
    "        # Define the output layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #input = input.unsqueeze(0)\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        #h0 = torch.zeros(self.num_layers, input.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initialize cell state\n",
    "        #c0 = torch.zeros(self.num_layers, input.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        \n",
    "        out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
    "        #out, self.hidden = self.lstm(input)\n",
    "        #out, (hn, cn) = self.lstm(input, (h0.detach(), c0.detach()))\n",
    "\n",
    "        \n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        #out = self.fc(out[-1].view(self.batch_size, -1))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        out = F.softmax(out, dim = 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, batch_size, output_dim, num_layers)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] Loss: 1.0935  Accuracy: 45.24 %\n",
      "Validation Epoch [1/1000] Loss: 1.1142  Accuracy: 36.55 %\n",
      "Epoch [101/1000] Loss: 0.8274  Accuracy: 72.94 %\n",
      "Validation Epoch [101/1000] Loss: 1.1508  Accuracy: 39.39 %\n",
      "Epoch [201/1000] Loss: 0.8038  Accuracy: 75.39 %\n",
      "Validation Epoch [201/1000] Loss: 1.1679  Accuracy: 37.73 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-92826bab9019>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Perform backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# Calling the step function on an Optimiser makes an update to its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yanxi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yanxi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# store all losses for visualisation\n",
    "all_losses = []\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "v_losses = []\n",
    "\"\"\"\n",
    "Train the neural network\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# train a neural network\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialise hidden state\n",
    "    model.hidden = model.init_hidden()\n",
    "\n",
    "    # Perform forward pass: compute predicted y by passing x to the model.\n",
    "    Y_pred = model(X_train)\n",
    "    \n",
    "    Y_pred_validate = model(X_validate)\n",
    "    \n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    #loss_func = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_func(Y_pred, Y_train)\n",
    "    all_losses.append(loss.item())\n",
    "    \n",
    "        \n",
    "    lossv = loss_func(Y_pred_validate,Y_validate)\n",
    "    v_losses.append(lossv.item())\n",
    "\n",
    "\n",
    "    # Clear the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Perform backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimiser makes an update to its\n",
    "    # parameters\n",
    "    optimiser.step()\n",
    "    \n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        # convert three-column predicted Y values to one column for comparison\n",
    "        _, predicted = torch.max(Y_pred, 1)\n",
    "\n",
    "        # calculate and print accuracy\n",
    "        total = predicted.size(0)\n",
    "        correct = predicted.data.numpy() == Y_train.data.numpy()\n",
    "\n",
    "        #print('Epoch [%d/%d] Loss: %.4f  Accuracy: %.2f %%'\n",
    "        #      % (epoch + 1, num_epochs, loss.item(), 100 * sum(correct)/total))\n",
    "        \n",
    "        print('Epoch [%d/%d] Loss: %.4f  Accuracy: %.2f %%'\n",
    "              % (epoch + 1, num_epochs, loss.item(), 100 * sum(correct)/total))\n",
    "        \n",
    "        \n",
    "        # validate set\n",
    "        _, predictedv = torch.max(Y_pred_validate, 1)\n",
    "                \n",
    "        totalv = predictedv.size(0)\n",
    "        correctv = predictedv.data.numpy() == Y_validate.data.numpy()\n",
    "        \n",
    "        print('Validation Epoch [%d/%d] Loss: %.4f  Accuracy: %.2f %%'\n",
    "              % (epoch + 1, num_epochs, lossv.item(), 100 * sum(correctv)/totalv))\n",
    "        \n",
    "\"\"\"\n",
    "Test the neural network\n",
    "\n",
    "Pass testing data to the built neural network and get its performance\n",
    "\"\"\"\n",
    "# test the neural network using testing data\n",
    "# It is actually performing a forward pass computation of predicted y\n",
    "# by passing x to the model.\n",
    "# Here, Y_pred_test contains three columns, where the index of the\n",
    "# max column indicates the class of the instance\n",
    "Y_pred_test = model(X_test)\n",
    "# get prediction\n",
    "# convert three-column predicted Y values to one column for comparison\n",
    "_, predicted_test = torch.max(Y_pred_test, 1)\n",
    "\n",
    "# calculate accuracy\n",
    "total_test = predicted_test.size(0)\n",
    "correct_test = sum(predicted_test.data.numpy() == Y_test.data.numpy())\n",
    "print('Testing Accuracy: %.2f %%' % (100 * correct_test / total_test))\n",
    "\n",
    "#print loss fig\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss (CrossEntropyLoss)\")\n",
    "\n",
    "plt.plot(all_losses,label=\"Train_losses\")\n",
    "plt.plot(v_losses, label=\"Validate_losses\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
