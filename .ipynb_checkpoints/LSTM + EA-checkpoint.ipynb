{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file implements a hybrid of GA and LSTM for the image manipulation-eye gaze timeseries data set.\n",
    "It is designed to solve the classification problem about predicting if a picture is manipulated based on a sequence of \n",
    "data on their eye gaze.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: load data\n",
    "\"\"\"\n",
    "# load all data\n",
    "data = pd.read_excel('Caldwell_ImageManipulation-EyeGaze_DataSetCombined.xlsx',\n",
    "                        sheet_name='data')\n",
    "\n",
    "data = data[[\"participant\", \"image\", \"image manipulated\", \"vote\"]]\n",
    "\n",
    "data_extended = pd.read_csv('Caldwell_Manip_Images_10-14_TimeSeries.csv')\n",
    "# rename columns to make them align\n",
    "data_extended = data_extended.rename(index=str, columns={'Participant_ID': 'participant', 'Image_ID': 'image'})\n",
    "data = pd.merge(data_extended, data, how=\"left\", on=[\"participant\", \"image\"])    # join the dataframes\n",
    "data = data.sort_values(by=[\"Start Time\"])\n",
    "\n",
    "# Min-Max scaling normalization\n",
    "for column in range(data.shape[1] - 2):\n",
    "    temp = data.iloc[:, column]  \n",
    "    ma = temp.max()\n",
    "    mi = temp.min()\n",
    "    data.iloc[:, column] = data.iloc[:, column].apply(lambda x: (x - mi) / (ma - mi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 2: Define the LSTM model\n",
    "\"\"\"\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"reference https://github.com/jessicayung/blog-code-snippets/blob/master/lstm-pytorch/lstm-baseline.py\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, batch, seq_len, output_dim, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch = batch\n",
    "        self.num_layers = num_layers\n",
    "        self.seq_len = seq_length\n",
    "    \n",
    "        # the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first = True)\n",
    "        \n",
    "        # the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        \"\"\"This is called each time a sequence is fully learned, then the hidden state has to be reinitialized\"\"\"\n",
    "        return (torch.zeros(self.num_layers, self.batch, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward pass through LSTM layer\"\"\"\n",
    "        # shape of self.hidden: (a, b), where a and b both have shape (num_layers, batch_size, hidden_dim).\n",
    "        # input has size batch * seq length * input_dim\n",
    "        lstm_out, self.hidden = self.lstm(input.view(self.batch, self.seq_len, -1))\n",
    "        \n",
    "        # Only take the output from the final timetep\n",
    "        y_pred = self.linear(lstm_out[:, -1, :]) \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 3: Define class for batching\n",
    "\"\"\"\n",
    "#use minibatch to preprocess data\n",
    "class PrepareData(Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        if not torch.is_tensor(X):\n",
    "            self.X = torch.from_numpy(X)\n",
    "        if not torch.is_tensor(y):\n",
    "            self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 2  # no. of output classes\n",
    "hidden_dim = 6  # no. of units in hidden state\n",
    "num_layers = 2  # number of LSTM layers\n",
    "batch = 10      # batch size\n",
    "seq_length = 10 # sequence length\n",
    "learning_rate = 0.01\n",
    "num_epochs = 200\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Define function for training\n",
    "\"\"\"\n",
    "def train(train_input, train_target, X_validate, Y_validate):\n",
    "    # hyperparameters\n",
    "    input_dim = train_input.shape[1]   # no. of input features\n",
    "\n",
    "    # create batches with size = batch_size*seq_length, build sequences later\n",
    "    train_batchs = PrepareData(X=np.array(train_input), y=np.array(train_target))\n",
    "    train_batchs = DataLoader(train_batchs, batch_size=batch * seq_length, shuffle=False)\n",
    "\n",
    "    model = LSTM(input_dim= input_dim, hidden_dim= hidden_dim, batch= batch, seq_len= seq_length, output_dim= output_dim, num_layers= num_layers)\n",
    "\n",
    "    loss_f = nn.CrossEntropyLoss()  # classification\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)   # minibatch gradient descent\n",
    "\n",
    "    losses = np.array([])\n",
    "    accuracies = np.array([])\n",
    "    previous_error = float(\"inf\")\n",
    "\n",
    "    \"\"\"Train and validate the model\"\"\"\n",
    "    for e in range(num_epochs):\n",
    "    \n",
    "        losses = np.array([])        # reinitialize each time to calculate average loss\n",
    "        accuracies = np.array([])    # reinitialize each time to calculate average accuracy\n",
    "\n",
    "        model.batch = batch          # restate batch size as it may has been changed by validation\n",
    "        model.seq_len = seq_length   # restate sequence length\n",
    "\n",
    "        counter = 0                  # count the number of batchs\n",
    "\n",
    "        # idea from https://conorsdatablog.wordpress.com/2018/05/03/up-and-running-with-pytorch-minibatching-dataloading-and-model-building/\n",
    "        for ix, (x, y) in enumerate(train_batchs):\n",
    "            # only learn from full batchs(size = batchsize *seqlength)\n",
    "            if x.shape[0] < batch * seq_length:     \n",
    "                continue\n",
    "\n",
    "            counter += 1\n",
    "            model.hidden = model.init_hidden()    # reinitialize hidden state each sequence\n",
    "\n",
    "            y = y.view(batch, seq_length)[:,-1]   # resize target into sequence\n",
    "\n",
    "            _X = Variable(x).float().view(batch, seq_length, -1)\n",
    "            _Y = Variable(y).long()\n",
    "\n",
    "            y_pred = model(_X)\n",
    "            loss = loss_f(y_pred, _Y)\n",
    "            losses = np.append(losses, (loss.item()))\n",
    "\n",
    "            # find the class from the max values in each row\n",
    "            _, predicted = torch.max(y_pred, dim = 1)\n",
    "\n",
    "            # calculate and print accuracy\n",
    "            total = predicted.size(0)\n",
    "            correct = predicted.data.numpy() == _Y.data.numpy()\n",
    "            accuracy = 100 * sum(correct)/total\n",
    "            accuracies = np.append(accuracies,accuracy)\n",
    "\n",
    "            optimizer.zero_grad()            # zero the gradients on each pass before the update\n",
    "            loss.backward()                  # backpropagate the loss through the model\n",
    "            optimizer.step()                 # update the gradients w.r.t the loss\n",
    "\n",
    "        if e % 10 == 0:                      # validate every 10 epochs  \n",
    "            print(\"Epoch: \", e, \" counter: \", counter)\n",
    "            print(\"loss\", sum(losses)/counter)\n",
    "            print(\"accuracies\", sum(accuracies)/counter)\n",
    "\n",
    "            # find the validation error, here sequence length is 1, batch size is the length of validation data\n",
    "            model.batch = X_validate.shape[0]\n",
    "            model.seq_len = 1\n",
    "            validate_y_pred = model(X_validate)\n",
    "            validate_loss = loss_f(validate_y_pred, Y_validate)\n",
    "            _, predicted = torch.max(validate_y_pred, dim = 1)\n",
    "            total = predicted.size(0)\n",
    "            correct = predicted.data.numpy() == Y_validate.data.numpy()\n",
    "            accuracy = 100 * sum(correct)/total\n",
    "            print(\"Validate\", validate_loss.item(),accuracy)\n",
    "\n",
    "            # terminate if validation loss is higher than previous two runs\n",
    "            if validate_loss > previous_error:\n",
    "                print(\"terminated: at epoch \", e)\n",
    "                break\n",
    "            previous_error = validate_loss\n",
    "            \n",
    "    return previous_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Fixations_ID', 'participant', 'image', 'X Pos', 'Y Pos', 'Start Time',\n",
      "       'Stop Time', 'Duration', 'Samples in Fixation'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 5: Using Genetic Algorithm to select features, reference: lab8, only up to the validation stage\n",
    "\"\"\"\n",
    "# define GA settings\n",
    "DNA_SIZE = 9             # number of bits in DNA\n",
    "POP_SIZE = 10             # population size\n",
    "CROSS_RATE = 0.8         # DNA crossover probability\n",
    "MUTATION_RATE = 0.002    # mutation probability\n",
    "N_GENERATIONS = 5        # generation size\n",
    "features = data.columns[:9]\n",
    "print(features)\n",
    "\n",
    "# define population select function based on fitness value\n",
    "# population with higher fitness value has higher chance to be selected, from lab8\n",
    "def select(pop, fitness):\n",
    "    idx = np.random.choice(np.arange(POP_SIZE+1), size=POP_SIZE + 1, replace=True,\n",
    "                           p=fitness/(sum(fitness)))\n",
    "    return pop[idx]\n",
    "\n",
    "# define mutation function, from lab8\n",
    "def mutate(child):\n",
    "    for point in range(DNA_SIZE):\n",
    "        if np.random.rand() < MUTATION_RATE:\n",
    "            child[point] = 1 if child[point] == 0 else 0\n",
    "    return child\n",
    "\n",
    "# define gene crossover function, from lab8\n",
    "def crossover(parent, pop):\n",
    "    if np.random.rand() < CROSS_RATE:\n",
    "        # randomly select another individual from population\n",
    "        i = np.random.randint(0, POP_SIZE, size=1)    \n",
    "        # choose crossover points(bits)\n",
    "        cross_points = np.random.randint(0, 2, size=DNA_SIZE).astype(np.bool)\n",
    "        # produce one child\n",
    "        parent[cross_points] = pop[i, cross_points]  \n",
    "    return parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to extract feature from DNA\n",
    "def extract(dna):\n",
    "    extracted = []\n",
    "    for i in range(len(dna)):\n",
    "        if dna[i] == 1:\n",
    "            extracted.append(features[i])\n",
    "    return extracted\n",
    "\n",
    "# define functions to create data from the features selected.\n",
    "def create_data(extracted):\n",
    "    # for speed, only take 1/10 of data\n",
    "    dummy_data = data[extracted]. iloc[[i for i in range(len(data)) if i % 10 == 0]]\n",
    "    traindata = dummy_data.loc[data['image'] < 1]\n",
    "    validatedata = dummy_data.loc[data['image'] == 1]\n",
    "\n",
    "    # separate the data into input and target\n",
    "    t_input = traindata.iloc[:, :len(extracted)]\n",
    "    t_target = traindata.iloc[:, -2]\n",
    "\n",
    "    v_input = validatedata.iloc[:, :len(extracted)]\n",
    "    v_target = validatedata.iloc[:, -2]\n",
    "\n",
    "    X_t = Variable(torch.Tensor(t_input.values).float())\n",
    "    Y_t = Variable(torch.Tensor(t_target.values).long())\n",
    "\n",
    "    X_v = Variable(torch.Tensor(v_input.values).float())\n",
    "    Y_v = Variable(torch.Tensor(v_target.values).long())\n",
    "    return X_t, Y_t, X_v, Y_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Generation  0 -------------\n",
      "examining:  [1 0 1 0 1 1 0 1 0]\n",
      "Epoch:  0  counter:  25\n",
      "loss 0.31864934906363485\n",
      "accuracies 88.0\n",
      "Validate 0.263240784406662 100.0\n",
      "Epoch:  10  counter:  25\n",
      "loss 0.0006949081388302147\n",
      "accuracies 100.0\n",
      "Validate 0.051680855453014374 100.0\n",
      "Epoch:  20  counter:  25\n",
      "loss 0.00026721382047981026\n",
      "accuracies 100.0\n",
      "Validate 0.036379266530275345 100.0\n",
      "Epoch:  30  counter:  25\n",
      "loss 0.00014556694077327847\n",
      "accuracies 100.0\n",
      "Validate 0.028918417170643806 100.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-47479a21053d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# use selected features to train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# add the loss related value to the fitness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-112-e8bd1e16eff4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_input, train_target, X_validate, Y_validate)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m            \u001b[1;31m# zero the gradients on each pass before the update\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                  \u001b[1;31m# backpropagate the loss through the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;31m# update the gradients w.r.t the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yanxi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yanxi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 6: Train the hybrid model\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the population DNA, add all 1 to the population\n",
    "pop = np.random.randint(2, size=(POP_SIZE, DNA_SIZE))\n",
    "pop = np.append(pop, [[1]*9],axis = 0)\n",
    "\n",
    "\n",
    "for t in range(N_GENERATIONS):\n",
    "    print('------------- Generation ', t,'-------------')\n",
    "    # fitness values for all populations\n",
    "    fitness = []\n",
    "    fit=np.array([])\n",
    "    for p in pop:\n",
    "        print(\"examining: \",p )\n",
    "        features_extracted = extract(p)\n",
    "        \n",
    "        X_t, Y_t, X_v, Y_v = create_data(features_extracted)\n",
    "              \n",
    "        # use selected features to train the model\n",
    "        loss = train(X_t, Y_t, X_v, Y_v)\n",
    "        \n",
    "        # add the loss related value to the fitness\n",
    "        fitness.append(loss)\n",
    "    \n",
    "    # select parent 1 index\n",
    "    p1 = fitness.index(min(fitness))\n",
    "    if min(fitness) < 0.1:\n",
    "        print('End-----------', pop[p1], \"fit: \", fitness[p1])\n",
    "        break\n",
    "              \n",
    "    selected_pop = select(pop, fitness)\n",
    "    selected_pop_copy = selected_pop.copy()\n",
    "    for parent in selected_pop:\n",
    "        child = crossover(parent, selected_pop_copy)       \n",
    "        child = mutate(child)\n",
    "        parent[:] = child\n",
    "    if t == N_GENERATIONS -1:\n",
    "        print('End-----------', pop[p1], \"fit: \", fitness[p1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
